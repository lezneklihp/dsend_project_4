{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import third-party packages.\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from lightgbm import LGBMClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, balanced_accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import warnings\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read in preprocessed data on trees.\n",
    "df = pd.read_csv('./data/data_preprocessed/features.csv') #, compression='gzip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x10e98fe50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x133d93af0>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x139fb3f70>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x133e3b460>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = df.loc[:, 'steward_Alive':'n_neighbors_two or more neighbors']\n",
    "y = df.loc[:, 'health_Dead|Stump':]\n",
    "\n",
    "# Split into training, testing, and validation datasets.\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x,\n",
    "                                                      y,\n",
    "                                                      test_size=0.5,\n",
    "                                                      random_state=34,\n",
    "                                                      shuffle=True,\n",
    "                                                      stratify=y)\n",
    "\n",
    "x_training, x_testing, y_training, y_testing = train_test_split(x_train,\n",
    "                                                                y_train,\n",
    "                                                                test_size=0.5,\n",
    "                                                                random_state=34,\n",
    "                                                                shuffle=True,\n",
    "                                                                stratify=y_train)\n",
    "\n",
    "# Check class imbalance of targets.\n",
    "y_training.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target_first_class = len(y_train.loc[y_train['health_Dead|Stump'] == 0]) / len(y_train)\n",
    "target_second_class = len(y_train.loc[y_train['health_Fair'] == 0]) / len(y_train)\n",
    "target_third_class = len(y_train.loc[y_train['health_Good'] == 0]) / len(y_train)\n",
    "target_fourth_class = len(y_train.loc[y_train['health_Poor'] == 0]) / len(y_train)\n",
    "\n",
    "\n",
    "class_weight_orig = [{0:target_first_class, 1:1-target_first_class},\n",
    "                     {0:target_second_class, 1:1-target_second_class},\n",
    "                     {0:target_third_class, 1:1-target_third_class},\n",
    "                     {0:target_fourth_class, 1:1-target_fourth_class}]\n",
    "\n",
    "class_weight_manip = [{0:target_first_class, 1:1-target_first_class},\n",
    "                      {0:1-target_second_class-0.1, 1:target_second_class+0.1},\n",
    "                      {0:target_third_class, 1:1-target_third_class},\n",
    "                      {0:1-target_fourth_class, 1:target_fourth_class}]\n",
    "\n",
    "print(class_weight_orig)\n",
    "print(class_weight_manip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x133d0c130>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1339f18b0>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x133b8f0d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x133bc0820>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ros = RandomOverSampler(sampling_strategy='minority', random_state=34)\n",
    "\n",
    "x_training_ros, y_training_ros = ros.fit_resample(x_training.values, y_training.values)\n",
    "\n",
    "pd.DataFrame(y_training_ros, columns=df.columns[-3:]).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f9753fd797c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_poorfair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'health_Poor|Fair'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'health_Dead|Stump'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx_pf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_poorfair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'steward_Alive'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'n_neighbors_two or more neighbors'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_pf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_poorfair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'health_Dead|Stump'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_new' is not defined"
     ]
    }
   ],
   "source": [
    "df_poorfair = df.loc[(df['health_Poor|Fair']==1) | (df_new['health_Dead|Stump']==0)].copy().reindex()\n",
    "\n",
    "x_pf = df_poorfair.loc[:, 'steward_Alive':'n_neighbors_two or more neighbors']\n",
    "y_pf = df_poorfair.loc[:, 'health_Dead|Stump':]\n",
    "\n",
    "x_train_pf, x_test_pf, y_train_pf, y_test_pf = train_test_split(x_pf,\n",
    "                                                                y_pf,\n",
    "                                                                test_size=0.5,\n",
    "                                                                random_state=34,\n",
    "                                                                stratify=y_pf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enlist classifiers and their default parameters.\n",
    "classifiers = {\n",
    "    'xgb': XGBClassifier(n_estimators=100,\n",
    "                         objective='binary:logistic',\n",
    "                         max_depth=8,\n",
    "                         learning_rate=0.1,\n",
    "                         verbosity=None,\n",
    "                         booster=None,\n",
    "                         tree_method=None,\n",
    "                         gamma=1,\n",
    "                         min_child_weight=None,\n",
    "                         max_delta_step=None,\n",
    "                         subsample=0.8,\n",
    "                         colsample_bytree=None,\n",
    "                         colsample_bylevel=None,\n",
    "                         colsample_bynode=None,\n",
    "                         reg_alpha=None,\n",
    "                         reg_lambda=None,\n",
    "                         scale_pos_weight=None,\n",
    "                         base_score=None,\n",
    "                         num_parallel_tree=None,\n",
    "                         random_state=34,\n",
    "                         n_jobs=-1,\n",
    "                         monotone_constraints=None,\n",
    "                         interaction_constraints=None,\n",
    "                         importance_type='gain',\n",
    "                         validate_parameters=None),\n",
    "    'ada': AdaBoostClassifier(base_estimator=None,\n",
    "                              n_estimators=50,\n",
    "                              learning_rate=1.0,\n",
    "                              random_state=34,\n",
    "                              algorithm='SAMME.R'),\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_estimator': None,\n",
       " 'n_estimators': 50,\n",
       " 'estimator_params': (),\n",
       " 'learning_rate': 1.0,\n",
       " 'random_state': None,\n",
       " 'algorithm': 'SAMME.R'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AdaBoostClassifier().__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6c8594b6adfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     xgb_clfs[each] = {\n\u001b[0;32m----> 8\u001b[0;31m                 'trained_classifier': XGBClassifier(learning_rate=0.1,\n\u001b[0m\u001b[1;32m      9\u001b[0m                                                     \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                                     \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    816\u001b[0m                                 missing=self.missing, nthread=self.n_jobs)\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         self._Booster = train(xgb_options, train_dmatrix,\n\u001b[0m\u001b[1;32m    819\u001b[0m                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_boosting_rounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m                               \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     return _train_internal(params, dtrain,\n\u001b[0m\u001b[1;32m    209\u001b[0m                            \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[1;32m   1368\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run the XGBClassifier with default parameter settings.\n",
    "xgb_clfs = {}\n",
    "\n",
    "for each in y.columns:\n",
    "    ind = list(y.columns).index(each)\n",
    "\n",
    "    xgb_clfs[each] = {\n",
    "                'trained_classifier': XGBClassifier(learning_rate=0.1,\n",
    "                                                    gamma=1,\n",
    "                                                    n_estimators=3000,\n",
    "                                                    max_depth=8,\n",
    "                                                    subsample=0.8,\n",
    "                                                    random_state=34,\n",
    "                                                    nthread=-1) \\\n",
    "                                     .fit(x_training_ros,\n",
    "                                          y_training_ros[:, ind])\n",
    "                }\n",
    "    xgb_clfs[each]['predictions'] = xgb_clfs[each]['trained_classifier'].predict(x_testing.values)\n",
    "    xgb_clfs[each]['accuracy'] = accuracy_score(y_testing.values[:, ind],\n",
    "                                                xgb_clfs[each]['predictions'])\n",
    "    \n",
    "    print(each, ': ', xgb_clfs[each]['accuracy'])\n",
    "    print(classification_report(y_testing.values[:, ind], xgb_clfs[each]['predictions']))\n",
    "    print(average_precision_score(y_testing.values[:, ind], xgb_clfs[each]['predictions'], average=None))\n",
    "    print('f1_score:', f1_score(y_testing.values[:, ind], xgb_clfs[each]['predictions'], average='binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health_Dead|Stump :  0.9999958215989905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    228261\n",
      "           1       1.00      1.00      1.00     11065\n",
      "\n",
      "    accuracy                           1.00    239326\n",
      "   macro avg       1.00      1.00      1.00    239326\n",
      "weighted avg       1.00      1.00      1.00    239326\n",
      "\n",
      "0.9999138033445251\n",
      "f1_score: 0.9999548104297528\n",
      "health_Good :  0.8215822768942781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.23      0.37     54228\n",
      "           1       0.82      0.99      0.90    185098\n",
      "\n",
      "    accuracy                           0.82    239326\n",
      "   macro avg       0.87      0.61      0.63    239326\n",
      "weighted avg       0.84      0.82      0.78    239326\n",
      "\n",
      "0.8154654107101259\n",
      "f1_score: 0.8960073647243343\n",
      "health_Poor|Fair :  0.8215864552952876\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.99      0.90    196163\n",
      "           1       0.58      0.04      0.07     43163\n",
      "\n",
      "    accuracy                           0.82    239326\n",
      "   macro avg       0.70      0.52      0.49    239326\n",
      "weighted avg       0.78      0.82      0.75    239326\n",
      "\n",
      "0.19540820509712564\n",
      "f1_score: 0.07008297580416839\n"
     ]
    }
   ],
   "source": [
    "# Run the AdaBoostClassifier with default parameter settings.\n",
    "ada_clfs = {}\n",
    "\n",
    "for each in y.columns:\n",
    "    ind = list(y.columns).index(each)\n",
    "\n",
    "    ada_clfs[each] = {\n",
    "                'trained_classifier': AdaBoostClassifier(learning_rate=1.0,\n",
    "                                                         algorithm='SAMME.R',\n",
    "                                                         n_estimators=50,\n",
    "                                                         random_state=34) \\\n",
    "                                     .fit(x_training_ros,\n",
    "                                          y_training_ros[:, ind])\n",
    "                }\n",
    "    ada_clfs[each]['predictions'] = ada_clfs[each]['trained_classifier'].predict(x_testing.values)\n",
    "    ada_clfs[each]['accuracy'] = accuracy_score(y_testing.values[:, ind],\n",
    "                                                ada_clfs[each]['predictions'])\n",
    "    \n",
    "    print(each, ': ', ada_clfs[each]['accuracy'])\n",
    "    print(classification_report(y_testing.values[:, ind], ada_clfs[each]['predictions']))\n",
    "    print(average_precision_score(y_testing.values[:, ind], ada_clfs[each]['predictions'], average=None))\n",
    "    print('f1_score:', f1_score(y_testing.values[:, ind], ada_clfs[each]['predictions'], average='binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health_Dead|Stump :  0.9999958215989905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    228261\n",
      "           1       1.00      1.00      1.00     11065\n",
      "\n",
      "    accuracy                           1.00    239326\n",
      "   macro avg       1.00      1.00      1.00    239326\n",
      "weighted avg       1.00      1.00      1.00    239326\n",
      "\n",
      "0.9999138033445251\n",
      "f1_score: 0.9999548104297528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    7.9s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health_Good :  0.8211101175802044\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.23      0.37     54228\n",
      "           1       0.82      0.99      0.90    185098\n",
      "\n",
      "    accuracy                           0.82    239326\n",
      "   macro avg       0.86      0.61      0.63    239326\n",
      "weighted avg       0.84      0.82      0.78    239326\n",
      "\n",
      "0.8153325018108686\n",
      "f1_score: 0.895706507837907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    7.8s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health_Poor|Fair :  0.8210892255751568\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.99      0.90    196163\n",
      "           1       0.56      0.04      0.07     43163\n",
      "\n",
      "    accuracy                           0.82    239326\n",
      "   macro avg       0.69      0.52      0.49    239326\n",
      "weighted avg       0.78      0.82      0.75    239326\n",
      "\n",
      "0.19444098176209995\n",
      "f1_score: 0.06953800686687818\n"
     ]
    }
   ],
   "source": [
    "# Run the RandomForestClassifier with default parameter settings.\n",
    "rdf_clfs = {}\n",
    "\n",
    "for each in y.columns:\n",
    "    ind = list(y.columns).index(each)\n",
    "\n",
    "    rdf_clfs[each] = {\n",
    "                'trained_classifier': RandomForestClassifier(n_estimators=100,\n",
    "                                                             random_state=34,\n",
    "                                                             max_depth=None,\n",
    "                                                             max_samples=None,\n",
    "                                                             min_samples_split=2,\n",
    "                                                             min_samples_leaf=1,\n",
    "                                                             min_weight_fraction_leaf=0.0,\n",
    "                                                             max_features='auto',\n",
    "                                                             max_leaf_nodes=None,\n",
    "                                                             min_impurity_decrease=0.0,\n",
    "                                                             oob_score=False,\n",
    "                                                             bootstrap=True,\n",
    "                                                             n_jobs=-1,\n",
    "                                                             verbose=1,\n",
    "                                                             warm_start=False,\n",
    "                                                             class_weight=None,\n",
    "                                                             ccp_alpha=0.0) \\\n",
    "                                     .fit(x_training_ros,\n",
    "                                          y_training_ros[:, ind])\n",
    "                }\n",
    "    rdf_clfs[each]['predictions'] = rdf_clfs[each]['trained_classifier'].predict(x_testing.values)\n",
    "    rdf_clfs[each]['accuracy'] = accuracy_score(y_testing.values[:, ind],\n",
    "                                                rdf_clfs[each]['predictions'])\n",
    "    \n",
    "    print(each, ': ', rdf_clfs[each]['accuracy'])\n",
    "    print(classification_report(y_testing.values[:, ind], rdf_clfs[each]['predictions']))\n",
    "    print(average_precision_score(y_testing.values[:, ind], rdf_clfs[each]['predictions'], average=None))\n",
    "    print('f1_score:', f1_score(y_testing.values[:, ind], rdf_clfs[each]['predictions'], average='binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health_Dead|Stump :  0.9999958215989905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    228261\n",
      "           1       1.00      1.00      1.00     11065\n",
      "\n",
      "    accuracy                           1.00    239326\n",
      "   macro avg       1.00      1.00      1.00    239326\n",
      "weighted avg       1.00      1.00      1.00    239326\n",
      "\n",
      "0.9999138033445251\n",
      "f1_score: 0.9999548104297528\n",
      "health_Good :  0.8216742017164871\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.23      0.37     54228\n",
      "           1       0.82      0.99      0.90    185098\n",
      "\n",
      "    accuracy                           0.82    239326\n",
      "   macro avg       0.87      0.61      0.63    239326\n",
      "weighted avg       0.84      0.82      0.78    239326\n",
      "\n",
      "0.8153708267690787\n",
      "f1_score: 0.8960897935333074\n",
      "health_Poor|Fair :  0.8216783801174966\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.99      0.90    196163\n",
      "           1       0.59      0.04      0.07     43163\n",
      "\n",
      "    accuracy                           0.82    239326\n",
      "   macro avg       0.71      0.52      0.48    239326\n",
      "weighted avg       0.78      0.82      0.75    239326\n",
      "\n",
      "0.19526037747182512\n",
      "f1_score: 0.06824880466344999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "# Try algs.\n",
    "log_regr = {}\n",
    "\n",
    "for each in y.columns:\n",
    "    ind = list(y.columns).index(each)\n",
    "\n",
    "    log_regr[each] = {\n",
    "                'trained_classifier': RidgeClassifier() \\\n",
    "                                     .fit(x_training_ros,\n",
    "                                          y_training_ros[:, ind])\n",
    "                }\n",
    "    log_regr[each]['predictions'] = log_regr[each]['trained_classifier'].predict(x_testing.values)\n",
    "    log_regr[each]['accuracy'] = accuracy_score(y_testing.values[:, ind],\n",
    "                                                log_regr[each]['predictions'])\n",
    "    \n",
    "    print(each, ': ', log_regr[each]['accuracy'])\n",
    "    print(classification_report(y_testing.values[:, ind], log_regr[each]['predictions']))\n",
    "    print(average_precision_score(y_testing.values[:, ind], log_regr[each]['predictions'], average=None))\n",
    "    print('f1_score:', f1_score(y_testing.values[:, ind], log_regr[each]['predictions'], average='binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health_Dead|Stump :  0.9999958215989905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    228261\n",
      "           1       1.00      1.00      1.00     11065\n",
      "\n",
      "    accuracy                           1.00    239326\n",
      "   macro avg       1.00      1.00      1.00    239326\n",
      "weighted avg       1.00      1.00      1.00    239326\n",
      "\n",
      "0.9999138033445251\n",
      "f1_score: 0.9999548104297528\n",
      "health_Good :  0.8217619481376867\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.23      0.37     54228\n",
      "           1       0.82      0.99      0.90    185098\n",
      "\n",
      "    accuracy                           0.82    239326\n",
      "   macro avg       0.87      0.61      0.63    239326\n",
      "weighted avg       0.84      0.82      0.78    239326\n",
      "\n",
      "0.8151910036133663\n",
      "f1_score: 0.8961861674036685\n",
      "health_Poor|Fair :  0.8216198825033636\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.99      0.90    196163\n",
      "           1       0.59      0.03      0.07     43163\n",
      "\n",
      "    accuracy                           0.82    239326\n",
      "   macro avg       0.71      0.51      0.48    239326\n",
      "weighted avg       0.78      0.82      0.75    239326\n",
      "\n",
      "0.1947399833315145\n",
      "f1_score: 0.06586289140281394\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Try algs.\n",
    "mlp_clfs = {}\n",
    "\n",
    "for each in y.columns:\n",
    "    ind = list(y.columns).index(each)\n",
    "\n",
    "    mlp_clfs[each] = {\n",
    "                'trained_classifier': MLPClassifier() \\\n",
    "                                     .fit(x_training_ros,\n",
    "                                          y_training_ros[:, ind])\n",
    "                }\n",
    "    mlp_clfs[each]['predictions'] = mlp_clfs[each]['trained_classifier'].predict(x_testing.values)\n",
    "    mlp_clfs[each]['accuracy'] = accuracy_score(y_testing.values[:, ind],\n",
    "                                                mlp_clfs[each]['predictions'])\n",
    "    \n",
    "    print(each, ': ', mlp_clfs[each]['accuracy'])\n",
    "    print(classification_report(y_testing.values[:, ind], mlp_clfs[each]['predictions']))\n",
    "    print(average_precision_score(y_testing.values[:, ind], mlp_clfs[each]['predictions'], average=None))\n",
    "    print('f1_score:', f1_score(y_testing.values[:, ind], mlp_clfs[each]['predictions'], average='binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health_Dead|Stump :  0.9999958215989905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    228261\n",
      "           1       1.00      1.00      1.00     11065\n",
      "\n",
      "    accuracy                           1.00    239326\n",
      "   macro avg       1.00      1.00      1.00    239326\n",
      "weighted avg       1.00      1.00      1.00    239326\n",
      "\n",
      "0.9999138033445251\n",
      "f1_score: 0.9999548104297528\n",
      "health_Good :  0.8216825585185061\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.23      0.37     54228\n",
      "           1       0.82      0.99      0.90    185098\n",
      "\n",
      "    accuracy                           0.82    239326\n",
      "   macro avg       0.87      0.61      0.63    239326\n",
      "weighted avg       0.84      0.82      0.78    239326\n",
      "\n",
      "0.8153831217665312\n",
      "f1_score: 0.8960931451082749\n",
      "health_Poor|Fair :  0.8216867369195157\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.99      0.90    196163\n",
      "           1       0.59      0.04      0.07     43163\n",
      "\n",
      "    accuracy                           0.82    239326\n",
      "   macro avg       0.71      0.52      0.48    239326\n",
      "weighted avg       0.78      0.82      0.75    239326\n",
      "\n",
      "0.19530467289112663\n",
      "f1_score: 0.06841450370014626\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Try algs.\n",
    "log_regr = {}\n",
    "\n",
    "for each in y.columns:\n",
    "    ind = list(y.columns).index(each)\n",
    "\n",
    "    log_regr[each] = {\n",
    "                'trained_classifier': LinearSVC() \\\n",
    "                                     .fit(x_training_ros,\n",
    "                                          y_training_ros[:, ind])\n",
    "                }\n",
    "    log_regr[each]['predictions'] = log_regr[each]['trained_classifier'].predict(x_testing.values)\n",
    "    log_regr[each]['accuracy'] = accuracy_score(y_testing.values[:, ind],\n",
    "                                                log_regr[each]['predictions'])\n",
    "    \n",
    "    print(each, ': ', log_regr[each]['accuracy'])\n",
    "    print(classification_report(y_testing.values[:, ind], log_regr[each]['predictions']))\n",
    "    print(average_precision_score(y_testing.values[:, ind], log_regr[each]['predictions'], average=None))\n",
    "    print('f1_score:', f1_score(y_testing.values[:, ind], log_regr[each]['predictions'], average='binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health_Dead|Stump :  0.9999367408906883\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    163043\n",
      "           1       1.00      1.00      1.00      7904\n",
      "\n",
      "    accuracy                           1.00    170947\n",
      "   macro avg       1.00      1.00      1.00    170947\n",
      "weighted avg       1.00      1.00      1.00    170947\n",
      "\n",
      "0.9998793315476783\n",
      "f1_score: 0.9999367368887201\n",
      "[[163043      0]\n",
      " [     1   7903]]\n",
      "health_Good :  0.6178251473996208\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.25      0.38     38734\n",
      "           1       0.82      0.99      0.90    132213\n",
      "\n",
      "    accuracy                           0.82    170947\n",
      "   macro avg       0.85      0.62      0.64    170947\n",
      "weighted avg       0.83      0.82      0.78    170947\n",
      "\n",
      "0.8170654198036122\n",
      "f1_score: 0.8955720125194547\n",
      "[[  9510  29224]\n",
      " [  1305 130908]]\n",
      "health_Poor|Fair :  0.5211800271795006\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.99      0.90    140117\n",
      "           1       0.55      0.05      0.09     30830\n",
      "\n",
      "    accuracy                           0.82    170947\n",
      "   macro avg       0.69      0.52      0.50    170947\n",
      "weighted avg       0.78      0.82      0.76    170947\n",
      "\n",
      "0.1994613729952896\n",
      "f1_score: 0.09441907360180297\n",
      "[[138817   1300]\n",
      " [ 29238   1592]]\n"
     ]
    }
   ],
   "source": [
    "# Run LGBMClassifier with default parameter settings.\n",
    "lgbm_clfs = {}\n",
    "\n",
    "for each in y.columns:\n",
    "    ind = list(y.columns).index(each)\n",
    "\n",
    "    lgbm_clfs[each] = {\n",
    "                'trained_classifier': LGBMClassifier(boosting_type='dart',\n",
    "                                                     num_leaves=100,\n",
    "                                                     max_depth=-1,\n",
    "                                                     learning_rate=0.04,\n",
    "                                                     n_estimators=100,\n",
    "                                                     subsample_for_bin=200000,\n",
    "                                                     objective='binary',\n",
    "                                                     class_weight=None,\n",
    "                                                     min_split_gain=0.0,\n",
    "                                                     min_child_weight=1e-3,\n",
    "                                                     min_child_samples=20,\n",
    "                                                     subsample=1.0,\n",
    "                                                     subsample_freq=0,\n",
    "                                                     colsample_bytree=1.0,\n",
    "                                                     reg_alpha=0.0,\n",
    "                                                     reg_lambda=0.0,\n",
    "                                                     random_state=34,\n",
    "                                                     n_jobs=-1,\n",
    "                                                     silent=False,\n",
    "                                                     importance_type='split') \\\n",
    "                                     .fit(x_training_ros,\n",
    "                                          y_training_ros[:, ind])\n",
    "                }\n",
    "    lgbm_clfs[each]['predictions'] = lgbm_clfs[each]['trained_classifier'].predict(x_testing.values)\n",
    "    lgbm_clfs[each]['accuracy'] = balanced_accuracy_score(y_testing.values[:, ind],\n",
    "                                                          lgbm_clfs[each]['predictions'])\n",
    "    \n",
    "    print(each, ': ', lgbm_clfs[each]['accuracy'])\n",
    "    print(classification_report(y_testing.values[:, ind], lgbm_clfs[each]['predictions']))\n",
    "    print(average_precision_score(y_testing.values[:, ind], lgbm_clfs[each]['predictions'], average=None))\n",
    "    print('f1_score:', f1_score(y_testing.values[:, ind], lgbm_clfs[each]['predictions'], average='binary'))\n",
    "    print(confusion_matrix(y_testing.values[:, ind], lgbm_clfs[each]['predictions']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8177388313336882\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "health_Dead|Stump       1.00      1.00      1.00      7904\n",
      "      health_Good       0.82      0.99      0.90    132213\n",
      " health_Poor|Fair       0.55      0.05      0.10     30830\n",
      "\n",
      "        micro avg       0.82      0.82      0.82    170947\n",
      "        macro avg       0.79      0.68      0.66    170947\n",
      "     weighted avg       0.78      0.82      0.76    170947\n",
      "      samples avg       0.82      0.82      0.82    170947\n",
      "\n",
      "0.7128446791359935\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "tes = MultiOutputClassifier(MLPClassifier()).fit(x_training_ros, y_training_ros)\n",
    "\n",
    "preds = tes.predict(x_testing.values)\n",
    "acc = accuracy_score(y_testing.values, preds)\n",
    "\n",
    "print(acc)\n",
    "print(classification_report(y_testing.values, preds, target_names=y.columns))\n",
    "print(average_precision_score(y_testing.values, preds, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tes = np.array([[1,0,0,0,0,1,0,0,1,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,0]])\n",
    "\n",
    "tes = pd.Series([1,0,0,0,0,1,0,0,1,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,0])\n",
    "\n",
    "def classify_tree(tree, classifiers, targets):\n",
    "    # tree = np.array([[1,0,0,0,0,1,0,0,1,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,0]])\n",
    "    # classifiers = xgb_clfs\n",
    "    # targets = y\n",
    "\n",
    "    results = {\n",
    "        each: classifiers[each]['trained_classifier'].predict(tree)\n",
    "        for each in targets.columns\n",
    "        }\n",
    "\n",
    "    if (int(sum(results.values())) == 0) | (int(sum(results.values())) > 1):\n",
    "        print(\"This tree cannot be classified.\")\n",
    "    else:\n",
    "        if int(results['health_Dead|Stump']) == 1:\n",
    "            message = \"This is a dead or stump tree.\"\n",
    "        elif int(results['health_Good']) == 1:\n",
    "            message = \"This is a healthy tree.\"\n",
    "        else:\n",
    "            message = \"This is a tree in a poor or fair health condition.\"\n",
    "            \n",
    "    return results, message\n",
    "\n",
    "a, c = classify_tree(tes, xgb_clfs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{0: 0.6666666666666666, 1: 0.33333333333333337}, {0: 0.6666666666666666, 1: 0.33333333333333337}, {0: 0.6666666666666666, 1: 0.33333333333333337}]\n",
      "[{0: 0.2, 1: 0.8}, {0: 0.2, 1: 0.8}, {0: 0.2, 1: 0.8}]\n"
     ]
    }
   ],
   "source": [
    "target_first_class = len(y_train_smote[:, 0][y_train_smote[:, 0] == 0]) / len(y_train_smote)\n",
    "target_second_class = len(y_train_smote[:, 1][y_train_smote[:, 1] == 0]) / len(y_train_smote)\n",
    "target_third_class = len(y_train_smote[:, 2][y_train_smote[:, 2] == 0]) / len(y_train_smote)\n",
    "\n",
    "class_weight_orig = [{0:target_first_class, 1:1-target_first_class},\n",
    "                     {0:target_second_class, 1:1-target_second_class},\n",
    "                     {0:target_third_class, 1:1-target_third_class}]\n",
    "\n",
    "class_weight_manip = [{0:0.2, 1:0.8},\n",
    "                      {0:0.2, 1:0.8},\n",
    "                      {0:0.2, 1:0.8}]\n",
    "\n",
    "print(class_weight_orig)\n",
    "print(class_weight_manip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8228047289510784\n"
     ]
    }
   ],
   "source": [
    "rdf_clf = RandomForestClassifier(class_weight=class_weight_manip, n_jobs=-1)\n",
    "\n",
    "rdf_clf.fit(x_train_ros, y_train_ros)\n",
    "y_pred = rdf_clf.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### START HYPERPARAMETER TUNING FOR TARGET: health_Dead|Stump ###########################\n",
      "F1 score:  1.0\n",
      "Updated parameter setting: \n",
      "{'boosting_type': 'dart', 'num_leaves': 31, 'max_depth': -1, 'learning_rate': 0.1, 'n_estimators': 300, 'subsample_for_bin': 200000, 'objective': 'binary', 'class_weight': None, 'min_split_gain': 0.2, 'min_child_weight': 0.001, 'min_child_samples': 20, 'subsample': 1.0, 'subsample_freq': 0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'random_state': 34, 'n_jobs': -1, 'silent': True, 'importance_type': 'split'}\n",
      "######### FINISH #####################################################################\n",
      "######### START HYPERPARAMETER TUNING FOR TARGET: health_Good ###########################\n",
      "F1 score:  0.913399874894079\n",
      "Updated parameter setting: \n",
      "{'boosting_type': 'dart', 'num_leaves': 31, 'max_depth': -1, 'learning_rate': 0.1, 'n_estimators': 300, 'subsample_for_bin': 200000, 'objective': 'binary', 'class_weight': None, 'min_split_gain': 0.2, 'min_child_weight': 0.001, 'min_child_samples': 20, 'subsample': 1.0, 'subsample_freq': 0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'random_state': 34, 'n_jobs': -1, 'silent': True, 'importance_type': 'split'}\n",
      "######### FINISH #####################################################################\n",
      "######### START HYPERPARAMETER TUNING FOR TARGET: health_Poor|Fair ###########################\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c92285525999>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     gridsearch_lgbm_clfs[each] = {\n\u001b[0;32m---> 39\u001b[0;31m                         'classifier': GridSearchCV(estimator=val['trained_classifier'],\n\u001b[0m\u001b[1;32m     40\u001b[0m                                                              \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameter_grid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                                                              \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    680\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[1;32m    683\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parameter_grid = [\n",
    "      #{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "                  {\n",
    "               'learning_rate': [0.1, 0.5, 0.8],\n",
    "               'n_estimators': [300, 1000, 3000, 5000]\n",
    "               }\n",
    "             ]\n",
    "\n",
    "default_parameters = {'boosting_type': 'dart',\n",
    "                      'num_leaves': 31,\n",
    "                      'max_depth': -1,\n",
    "                      'learning_rate': 0.1,\n",
    "                      'n_estimators': 100,\n",
    "                      'subsample_for_bin': 200000,\n",
    "                      'objective': 'binary',\n",
    "                      'class_weight': None,\n",
    "                      'min_split_gain': 0.2,\n",
    "                      'min_child_weight': 1e-3,\n",
    "                      'min_child_samples': 20,\n",
    "                      'subsample': 1.0,\n",
    "                      'subsample_freq': 0,\n",
    "                      'colsample_bytree': 1.0,\n",
    "                      'reg_alpha': 0.0,\n",
    "                      'reg_lambda': 0.0,\n",
    "                      'random_state': 34,\n",
    "                      'n_jobs': -1,\n",
    "                      'silent': True,\n",
    "                      'importance_type': 'split'}\n",
    "\n",
    "gridsearch_lgbm_clfs = {}\n",
    "\n",
    "for each, val in lgbm_clfs.items():\n",
    "    \n",
    "    ind = list(y.columns).index(each)\n",
    "    \n",
    "    print('######### START HYPERPARAMETER TUNING FOR TARGET:', each, '###########################')\n",
    "    \n",
    "    gridsearch_lgbm_clfs[each] = {\n",
    "                        'classifier': GridSearchCV(estimator=val['trained_classifier'],\n",
    "                                                             param_grid=parameter_grid,\n",
    "                                                             cv=5,\n",
    "                                                             scoring='f1',\n",
    "                                                             refit=True,\n",
    "                                                             verbose=0,\n",
    "                                                             error_score=0,\n",
    "                                                             return_train_score=False,\n",
    "                                                             n_jobs=-1) \\\n",
    "                                     .fit(x_training_ros,\n",
    "                                          y_training_ros[:, ind],\n",
    "                                          verbose=0)\n",
    "                        }\n",
    "    \n",
    "    print('F1 score: ', gridsearch_lgbm_clfs[each]['classifier'].best_score_)\n",
    "    \n",
    "    # Update the set of default parameters.\n",
    "    default_parameters['learning_rate'] = gridsearch_lgbm_clfs[each]['classifier'].best_params_['learning_rate']\n",
    "    default_parameters['n_estimators'] = gridsearch_lgbm_clfs[each]['classifier'].best_params_['n_estimators']    \n",
    "    \n",
    "    print('Updated parameter setting: ')\n",
    "    gridsearch_lgbm_clfs[each]['best_params'] = default_parameters\n",
    "    print(default_parameters)\n",
    "    print('######### FINISH #####################################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### START HYPERPARAMETER TUNING FOR TARGET: health_Dead|Stump ###########################\n",
      "F1 score:  1.0\n",
      "Updated parameter setting: \n",
      "{'boosting_type': 'dart', 'num_leaves': 31, 'max_depth': -1, 'learning_rate': 0.1, 'n_estimators': 300, 'subsample_for_bin': 200000, 'objective': 'binary', 'class_weight': None, 'min_split_gain': 0.2, 'min_child_weight': 0.001, 'min_child_samples': 20, 'subsample': 1.0, 'subsample_freq': 0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'random_state': 34, 'n_jobs': -1, 'silent': True, 'importance_type': 'split'}\n",
      "######### FINISH #####################################################################\n",
      "######### START HYPERPARAMETER TUNING FOR TARGET: health_Good ###########################\n",
      "F1 score:  0.8944577019175042\n",
      "Updated parameter setting: \n",
      "{'boosting_type': 'dart', 'num_leaves': 31, 'max_depth': -1, 'learning_rate': 0.5, 'n_estimators': 1000, 'subsample_for_bin': 200000, 'objective': 'binary', 'class_weight': None, 'min_split_gain': 0.2, 'min_child_weight': 0.001, 'min_child_samples': 20, 'subsample': 1.0, 'subsample_freq': 0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'random_state': 34, 'n_jobs': -1, 'silent': True, 'importance_type': 'split'}\n",
      "######### FINISH #####################################################################\n",
      "######### START HYPERPARAMETER TUNING FOR TARGET: health_Poor|Fair ###########################\n",
      "F1 score:  0.11558169706725455\n",
      "Updated parameter setting: \n",
      "{'boosting_type': 'dart', 'num_leaves': 31, 'max_depth': -1, 'learning_rate': 0.5, 'n_estimators': 3000, 'subsample_for_bin': 200000, 'objective': 'binary', 'class_weight': None, 'min_split_gain': 0.2, 'min_child_weight': 0.001, 'min_child_samples': 20, 'subsample': 1.0, 'subsample_freq': 0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'random_state': 34, 'n_jobs': -1, 'silent': True, 'importance_type': 'split'}\n",
      "######### FINISH #####################################################################\n"
     ]
    }
   ],
   "source": [
    "parameter_grid = [\n",
    "      #{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "                  {\n",
    "               'boosting_type': ['dart'],\n",
    "               'num_leaves': [31, 100, 1000],\n",
    "               'learning_rate': [0.1, 0.5],\n",
    "               'n_estimators': [300, 1000, 3000],\n",
    "               'objective': ['binary'],\n",
    "               'random_state': [34]\n",
    "               }\n",
    "             ]\n",
    "\n",
    "default_parameters = {'boosting_type': 'dart',\n",
    "                      'num_leaves': 31,\n",
    "                      'max_depth': -1,\n",
    "                      'learning_rate': 0.1,\n",
    "                      'n_estimators': 100,\n",
    "                      'subsample_for_bin': 200000,\n",
    "                      'objective': 'binary',\n",
    "                      'class_weight': None,\n",
    "                      'min_split_gain': 0.2,\n",
    "                      'min_child_weight': 1e-3,\n",
    "                      'min_child_samples': 20,\n",
    "                      'subsample': 1.0,\n",
    "                      'subsample_freq': 0,\n",
    "                      'colsample_bytree': 1.0,\n",
    "                      'reg_alpha': 0.0,\n",
    "                      'reg_lambda': 0.0,\n",
    "                      'random_state': 34,\n",
    "                      'n_jobs': -1,\n",
    "                      'silent': True,\n",
    "                      'importance_type': 'split'}\n",
    "\n",
    "gridsearch_lgbm_clfs = {}\n",
    "\n",
    "for each, val in lgbm_clfs.items():\n",
    "    \n",
    "    ind = list(y.columns).index(each)\n",
    "    \n",
    "    print('######### START HYPERPARAMETER TUNING FOR TARGET:', each, '###########################')\n",
    "    \n",
    "    gridsearch_lgbm_clfs[each] = {\n",
    "                        'classifier': GridSearchCV(estimator=val['trained_classifier'],\n",
    "                                                             param_grid=parameter_grid,\n",
    "                                                             cv=2,\n",
    "                                                             scoring='f1',\n",
    "                                                             refit=True,\n",
    "                                                             verbose=0,\n",
    "                                                             error_score=0,\n",
    "                                                             return_train_score=False,\n",
    "                                                             n_jobs=-1) \\\n",
    "                                     .fit(x_training_ros,\n",
    "                                          y_training_ros[:, ind],\n",
    "                                          verbose=0)\n",
    "                        }\n",
    "    \n",
    "    print('F1 score: ', gridsearch_lgbm_clfs[each]['classifier'].best_score_)\n",
    "    \n",
    "    # Update the set of default parameters.\n",
    "    default_parameters['num_leaves'] = gridsearch_lgbm_clfs[each]['classifier'].best_params_['num_leaves']\n",
    "    default_parameters['learning_rate'] = gridsearch_lgbm_clfs[each]['classifier'].best_params_['learning_rate']\n",
    "    default_parameters['n_estimators'] = gridsearch_lgbm_clfs[each]['classifier'].best_params_['n_estimators']    \n",
    "    \n",
    "    print('Updated parameter setting: ')\n",
    "    gridsearch_lgbm_clfs[each]['best_params'] = default_parameters\n",
    "    print(default_parameters)\n",
    "    print('######### FINISH #####################################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### START HYPERPARAMETER TUNING FOR TARGET: health_Dead|Stump ###########################\n",
      "F1 score:  1.0\n",
      "Updated parameter setting: \n",
      "{'hidden_layer_sizes': (100,), 'activation': 'identity', 'solver': 'lbfgs', 'alpha': 0.0001, 'batch_size': 'auto', 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'power_t': 0.5, 'max_iter': 200, 'shuffle': True, 'tol': 0.0001, 'verbose': False, 'momentum': 0.9, 'nesterovs_momentum': True, 'early_stopping': True, 'validation_fraction': 0.1, 'random_state': 34, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-08, 'n_iter_no_change': 10, 'max_fun': 15000}\n",
      "######### FINISH #####################################################################\n",
      "######### START HYPERPARAMETER TUNING FOR TARGET: health_Good ###########################\n",
      "F1 score:  0.9045008187549601\n",
      "Updated parameter setting: \n",
      "{'hidden_layer_sizes': (100,), 'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.0001, 'batch_size': 'auto', 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'power_t': 0.5, 'max_iter': 200, 'shuffle': True, 'tol': 0.0001, 'verbose': False, 'momentum': 0.9, 'nesterovs_momentum': True, 'early_stopping': True, 'validation_fraction': 0.1, 'random_state': 34, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-08, 'n_iter_no_change': 10, 'max_fun': 15000}\n",
      "######### FINISH #####################################################################\n",
      "######### START HYPERPARAMETER TUNING FOR TARGET: health_Poor|Fair ###########################\n",
      "F1 score:  0.11707503527068443\n",
      "Updated parameter setting: \n",
      "{'hidden_layer_sizes': (100,), 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.0001, 'batch_size': 'auto', 'learning_rate': 'adaptive', 'learning_rate_init': 0.001, 'power_t': 0.5, 'max_iter': 200, 'shuffle': True, 'tol': 0.0001, 'verbose': False, 'momentum': 0.9, 'nesterovs_momentum': True, 'early_stopping': True, 'validation_fraction': 0.1, 'random_state': 34, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-08, 'n_iter_no_change': 10, 'max_fun': 15000}\n",
      "######### FINISH #####################################################################\n"
     ]
    }
   ],
   "source": [
    "parameter_grid = [\n",
    "                  {\n",
    "               'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "               'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "               'learning_rate': ['constant', 'invscaling', 'adaptive']\n",
    "               }\n",
    "             ]\n",
    "\n",
    "default_parameters = {'hidden_layer_sizes': (100,),\n",
    "                      'activation': 'relu',\n",
    "                      'solver': 'adam',\n",
    "                      'alpha': 0.0001,\n",
    "                      'batch_size': 'auto',\n",
    "                      'learning_rate': 'constant',\n",
    "                      'learning_rate_init': 0.001,\n",
    "                      'power_t': 0.5,\n",
    "                      'max_iter': 200,\n",
    "                      'shuffle': True,\n",
    "                      'tol': 1e-4,\n",
    "                      'verbose': False,\n",
    "                      'momentum': 0.9, # only applies if solver='sgd'\n",
    "                      'nesterovs_momentum': True, # only applies if solver='sgd'\n",
    "                      'early_stopping': True,\n",
    "                      'validation_fraction': 0.1,\n",
    "                      'random_state': 34,\n",
    "                      'beta_1': 0.9, # only applies if solver='adam'\n",
    "                      'beta_2': 0.999, # only applies if solver='adam'\n",
    "                      'epsilon': 1e-8, # only applies if solver='adam'\n",
    "                      'n_iter_no_change': 10, # only applies if solver='adam' or 'sgd'\n",
    "                      'max_fun': 15000}\n",
    "\n",
    "gridsearch_mlp_clfs = {}\n",
    "\n",
    "for each, val in mlp_clfs.items():\n",
    "    \n",
    "    ind = list(y.columns).index(each)\n",
    "    \n",
    "    print('######### START HYPERPARAMETER TUNING FOR TARGET:', each, '###########################')\n",
    "    \n",
    "    gridsearch_mlp_clfs[each] = {\n",
    "                        'classifier': GridSearchCV(estimator=val['trained_classifier'],\n",
    "                                                             param_grid=parameter_grid,\n",
    "                                                             cv=2,\n",
    "                                                             scoring='f1',\n",
    "                                                             refit=True,\n",
    "                                                             verbose=0,\n",
    "                                                             error_score=0,\n",
    "                                                             return_train_score=False,\n",
    "                                                             n_jobs=-1) \\\n",
    "                                     .fit(x_training_ros,\n",
    "                                          y_training_ros[:, ind])\n",
    "                        }\n",
    "    \n",
    "    print('F1 score: ', gridsearch_mlp_clfs[each]['classifier'].best_score_)\n",
    "    \n",
    "    # Update the set of default parameters.\n",
    "    default_parameters['activation'] = gridsearch_mlp_clfs[each]['classifier'].best_params_['activation']\n",
    "    default_parameters['solver'] = gridsearch_mlp_clfs[each]['classifier'].best_params_['solver']\n",
    "    default_parameters['learning_rate'] = gridsearch_mlp_clfs[each]['classifier'].best_params_['learning_rate']    \n",
    "    \n",
    "    print('Updated parameter setting: ')\n",
    "    gridsearch_mlp_clfs[each]['best_params'] = default_parameters\n",
    "    print(default_parameters)\n",
    "    print('######### FINISH #####################################################################')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with optimized hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health_Dead|Stump :  0.9999958215989905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    228261\n",
      "           1       1.00      1.00      1.00     11065\n",
      "\n",
      "    accuracy                           1.00    239326\n",
      "   macro avg       1.00      1.00      1.00    239326\n",
      "weighted avg       1.00      1.00      1.00    239326\n",
      "\n",
      "0.9999138033445251\n",
      "f1_score: 0.9999548104297528\n",
      "health_Good :  0.8216867369195157\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.23      0.37     54228\n",
      "           1       0.82      0.99      0.90    185098\n",
      "\n",
      "    accuracy                           0.82    239326\n",
      "   macro avg       0.87      0.61      0.63    239326\n",
      "weighted avg       0.84      0.82      0.78    239326\n",
      "\n",
      "0.8152667188228706\n",
      "f1_score: 0.8961191021574608\n",
      "health_Poor|Fair :  0.8216700233154777\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.99      0.90    196163\n",
      "           1       0.59      0.04      0.07     43163\n",
      "\n",
      "    accuracy                           0.82    239326\n",
      "   macro avg       0.71      0.52      0.48    239326\n",
      "weighted avg       0.78      0.82      0.75    239326\n",
      "\n",
      "0.19497794607776878\n",
      "f1_score: 0.06669728181241663\n"
     ]
    }
   ],
   "source": [
    "# Run LGBMClassifier with tuned parameter settings.\n",
    "lgbm_clfs = {}\n",
    "\n",
    "for each in y.columns:\n",
    "    ind = list(y.columns).index(each)\n",
    "\n",
    "    lgbm_clfs[each] = {\n",
    "                'trained_classifier': LGBMClassifier(**gridsearch_lgbm_clfs[each]['best_params']) \\\n",
    "                                     .fit(x_training_ros,\n",
    "                                          y_training_ros[:, ind])\n",
    "                }\n",
    "    lgbm_clfs[each]['predictions'] = lgbm_clfs[each]['trained_classifier'].predict(x_testing.values)\n",
    "    lgbm_clfs[each]['accuracy'] = accuracy_score(y_testing.values[:, ind],\n",
    "                                                lgbm_clfs[each]['predictions'])\n",
    "    \n",
    "    print(each, ': ', lgbm_clfs[each]['accuracy'])\n",
    "    print(classification_report(y_testing.values[:, ind], lgbm_clfs[each]['predictions']))\n",
    "    print(average_precision_score(y_testing.values[:, ind], lgbm_clfs[each]['predictions'], average=None))\n",
    "    print('f1_score:', f1_score(y_testing.values[:, ind], lgbm_clfs[each]['predictions'], average='binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health_Dead|Stump :  0.9999958215989905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    228261\n",
      "           1       1.00      1.00      1.00     11065\n",
      "\n",
      "    accuracy                           1.00    239326\n",
      "   macro avg       1.00      1.00      1.00    239326\n",
      "weighted avg       1.00      1.00      1.00    239326\n",
      "\n",
      "0.9999138033445251\n",
      "f1_score: 0.9999548104297528\n",
      "health_Good :  0.8216616665134586\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.23      0.37     54228\n",
      "           1       0.82      0.99      0.90    185098\n",
      "\n",
      "    accuracy                           0.82    239326\n",
      "   macro avg       0.87      0.61      0.63    239326\n",
      "weighted avg       0.84      0.82      0.78    239326\n",
      "\n",
      "0.8152119805653325\n",
      "f1_score: 0.8961125891164179\n",
      "health_Poor|Fair :  0.8214276760569266\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.99      0.90    196163\n",
      "           1       0.58      0.04      0.07     43163\n",
      "\n",
      "    accuracy                           0.82    239326\n",
      "   macro avg       0.70      0.52      0.49    239326\n",
      "weighted avg       0.78      0.82      0.75    239326\n",
      "\n",
      "0.19511892311037182\n",
      "f1_score: 0.0700654960071371\n"
     ]
    }
   ],
   "source": [
    "# Run LGBMClassifier with tuned parameter settings.\n",
    "mlp_clfs = {}\n",
    "\n",
    "for each in y.columns:\n",
    "    ind = list(y.columns).index(each)\n",
    "\n",
    "    mlp_clfs[each] = {\n",
    "                'trained_classifier': MLPClassifier(**gridsearch_mlp_clfs[each]['best_params']) \\\n",
    "                                     .fit(x_training_ros,\n",
    "                                          y_training_ros[:, ind])\n",
    "                }\n",
    "    mlp_clfs[each]['predictions'] = mlp_clfs[each]['trained_classifier'].predict(x_testing.values)\n",
    "    mlp_clfs[each]['accuracy'] = accuracy_score(y_testing.values[:, ind],\n",
    "                                                mlp_clfs[each]['predictions'])\n",
    "    \n",
    "    print(each, ': ', mlp_clfs[each]['accuracy'])\n",
    "    print(classification_report(y_testing.values[:, ind], mlp_clfs[each]['predictions']))\n",
    "    print(average_precision_score(y_testing.values[:, ind], mlp_clfs[each]['predictions'], average=None))\n",
    "    print('f1_score:', f1_score(y_testing.values[:, ind], mlp_clfs[each]['predictions'], average='binary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save / load current status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('xgb_clfs.pkl', 'wb') as handle:\n",
    "    pickle.dump(xgb_clfs, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gridsearch_lgbm_clfs.pkl', 'rb') as handle:\n",
    "    gridsearch_lgbm_clfs = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 231120\r\n",
      "drwxrwxr-x@ 24 phillip.kenzel  VF-ROOT\\Domain Users   768B May 31 23:56 \u001b[34m.\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   9 phillip.kenzel  VF-ROOT\\Domain Users   288B May 24 14:59 \u001b[34m..\u001b[m\u001b[m\r\n",
      "-rw-r--r--@  1 phillip.kenzel  VF-ROOT\\Domain Users   6.0K May 24 14:59 .DS_Store\r\n",
      "drwxrwxr-x@ 11 phillip.kenzel  VF-ROOT\\Domain Users   352B May 30 18:38 \u001b[34m.ipynb_checkpoints\u001b[m\u001b[m\r\n",
      "-rw-rw-r--@  1 phillip.kenzel  VF-ROOT\\Domain Users   227B May 23 21:52 README.md\r\n",
      "drwxrwxr-x@  5 phillip.kenzel  VF-ROOT\\Domain Users   160B May 23 21:52 \u001b[34marchive\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 phillip.kenzel  VF-ROOT\\Domain Users    94K May 24 16:00 clustering.ipynb\r\n",
      "drwxrwxr-x@  5 phillip.kenzel  VF-ROOT\\Domain Users   160B May 23 21:52 \u001b[34mdata\u001b[m\u001b[m\r\n",
      "drwxrwxr-x@  6 phillip.kenzel  VF-ROOT\\Domain Users   192B May 23 21:52 \u001b[34mdata_descriptions\u001b[m\u001b[m\r\n",
      "-rw-rw-r--@  1 phillip.kenzel  VF-ROOT\\Domain Users    19K May 30 20:47 eda_trees.ipynb\r\n",
      "-rw-rw-r--@  1 phillip.kenzel  VF-ROOT\\Domain Users   147K May 30 13:36 featureeng_trees.ipynb\r\n",
      "-rw-r--r--   1 phillip.kenzel  VF-ROOT\\Domain Users    15M May 31 20:44 gridsearch_lgbm_clfs.pkl\r\n",
      "-rw-r--r--   1 phillip.kenzel  VF-ROOT\\Domain Users   384K May 31 22:16 gridsearch_mlp_clfs.pkl\r\n",
      "-rw-r--r--   1 phillip.kenzel  VF-ROOT\\Domain Users    14M May 31 22:16 lgbm_clfs.pkl\r\n",
      "-rw-r--r--   1 phillip.kenzel  VF-ROOT\\Domain Users   5.7M May 31 22:16 mlp_clfs.pkl\r\n",
      "-rw-rw-r--@  1 phillip.kenzel  VF-ROOT\\Domain Users   237K May 23 21:52 model_trees.ipynb\r\n",
      "-rw-r--r--   1 phillip.kenzel  VF-ROOT\\Domain Users   119K May 30 20:50 modeling_trees-Copy1.ipynb\r\n",
      "-rw-rw-r--@  1 phillip.kenzel  VF-ROOT\\Domain Users    90K May 31 23:55 modeling_trees.ipynb\r\n",
      "-rw-rw-r--@  1 phillip.kenzel  VF-ROOT\\Domain Users    15M May 23 21:52 rdf.pkl\r\n",
      "-rw-rw-r--@  1 phillip.kenzel  VF-ROOT\\Domain Users   5.2M May 23 21:52 strassenbaumkatasterleipzig12032020.csv\r\n",
      "-rw-rw-r--@  1 phillip.kenzel  VF-ROOT\\Domain Users    20K May 23 21:52 test_voila.ipynb\r\n",
      "-rw-rw-r--@  1 phillip.kenzel  VF-ROOT\\Domain Users    26K May 30 18:44 webapp_trees.ipynb\r\n",
      "-rw-r--r--   1 phillip.kenzel  VF-ROOT\\Domain Users   7.9M May 30 13:38 xgb_clfs.pickle\r\n",
      "-rw-r--r--   1 phillip.kenzel  VF-ROOT\\Domain Users    49M May 31 23:56 xgb_clfs.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
